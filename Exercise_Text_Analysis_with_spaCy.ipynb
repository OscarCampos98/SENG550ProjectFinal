{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analysis with spaCy\n",
        "Compared to NLTK, Spacy is a more modern and efficient NLP toolkit that plays nicely with newer approaches like vector embeddings and transformer-based large language models.\n",
        "\n",
        "Lots more details at https://spacy.io/\n",
        "\n",
        "This notebook contains some basic demos, using data from the same source we'll use in this week's datathon."
      ],
      "metadata": {
        "id": "8iN9wS3CTYsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import altair as alt"
      ],
      "metadata": {
        "id": "_BKaFWlm2Qub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⇣ spaCy's default small English language model — comes loaded by default and does most of the basics\n",
        "#nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ⇣ spaCy's full English language model with word vectors (~560Mb download) - needed to play with embeddings, similarity, etc.\n",
        "!python -m spacy download en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')"
      ],
      "metadata": {
        "id": "cDx5tkw-2d8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running spaCy's initial processing pipeline (applied by calling `nlp()`) gives us a bunch of features we'd have needed to handle more manually with NLTK. Let's try that with just one song."
      ],
      "metadata": {
        "id": "lJGMtAGSZan2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A sample of 50 songs from one artist, drawn from Genius.com\n",
        "one_artist = pd.read_excel('https://drive.google.com/uc?export=download&id=1LIKWcgLHw19lS174dpA2sfhn3hA5ZPZh')\n",
        "one_artist.head()"
      ],
      "metadata": {
        "id": "6xQ8iL7V2Hdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_song = one_artist[one_artist['title'] == 'It’s All About the Pentiums'].iloc[0].lyrics\n",
        "one_song_doc = nlp(one_song)"
      ],
      "metadata": {
        "id": "bBmruUrx2lBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_song_doc"
      ],
      "metadata": {
        "id": "3HBIg9o4ZaWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting object gives us access to things like\n",
        "- Language detection"
      ],
      "metadata": {
        "id": "vEee2qfcamWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Language:\",one_song_doc.lang_)"
      ],
      "metadata": {
        "id": "XmQ4isBFZUBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenization"
      ],
      "metadata": {
        "id": "GY5aoX4Gaq55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\", [token.text for token in one_song_doc])"
      ],
      "metadata": {
        "id": "8XS9n4I9aN16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lemmatization (converting terms into their 'base forms')."
      ],
      "metadata": {
        "id": "tkyasGTxatEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Lemmatized:\", [token.lemma_ for token in one_song_doc])"
      ],
      "metadata": {
        "id": "eRcbESZBaR6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Helpers for handling stopwords and more (here we lemmatize, lowercase, remove stopwords, and keep only tokens composed of letters and numbers)"
      ],
      "metadata": {
        "id": "lyYMsmqja7YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Minus stopwords, etc.:\", [token.lemma_.lower() for token in one_song_doc if not token.is_stop and (token.is_alpha or token.like_num)])"
      ],
      "metadata": {
        "id": "1bEt6_VsaWWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Part-of-speech tagging.\n",
        "(SpaCy's parts of speech codes here: https://github.com/explosion/spaCy/blob/master/spacy/glossary.py)"
      ],
      "metadata": {
        "id": "xACpV5RRD_eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Parts of Speech:\", [token.pos_ + \": \" + token.text for token in one_song_doc])\n",
        "print(\"Just the Verbs:\", [token.lemma_ for token in one_song_doc if token.pos_ == \"VERB\"])\n",
        "print(\"Just the Nouns:\", [token.lemma_ for token in one_song_doc if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\"])"
      ],
      "metadata": {
        "id": "4Awnu4c1ajOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy can also extracts noun phrases and does entity detection.\n",
        "(We're doing all of this with the default base pipeline so results are hit-or-miss, but you have the ability to roll your own pipelines if you want better performance.)"
      ],
      "metadata": {
        "id": "EVya7kX4bO2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Noun phrases:\", [chunk.text for chunk in one_song_doc.noun_chunks ])\n",
        "print(\"Entities:\", [entity.label_ + \": \" + entity.text for entity in one_song_doc.ents])"
      ],
      "metadata": {
        "id": "BhF-7rbZ2tQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's also some basic visualization support via the `displayCy` package if you want to see entities inline, inspect the parse tree, etc."
      ],
      "metadata": {
        "id": "gcHFkY6PbpLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(one_song_doc,style='ent',jupyter=True)"
      ],
      "metadata": {
        "id": "QkN2OdD07kSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency plots\n",
        "Using these basics we can also start creating our own charts to examine the text."
      ],
      "metadata": {
        "id": "RuiZikwkfhUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renders a quick Altair frequency chart from a list of strings\n",
        "def frequency_chart(phrase_list, top=50, normalize=True, ymax=.1, color=None):\n",
        "  counter = Counter(phrase_list)\n",
        "  if(normalize):\n",
        "    total = sum(counter.values())\n",
        "    for k in counter:\n",
        "      counter[k] /= total\n",
        "  top_counts = counter.most_common(top)\n",
        "  chart = alt.Chart(pd.DataFrame(top_counts)).mark_bar()\n",
        "  if normalize:\n",
        "    chart = chart.encode(\n",
        "      x=alt.X('0:O',sort='-y',title='word'),\n",
        "      y=alt.Y('1:Q',title='% of words',axis=alt.Axis(format='%'),scale=alt.Scale(domain=[0,ymax])))\n",
        "  else:\n",
        "    chart = chart.encode(\n",
        "      x=alt.X('0:O',sort='-y',title='word'),\n",
        "      y=alt.Y('1:Q',title='count'))\n",
        "  if color:\n",
        "    chart = chart.configure_mark(color=color)\n",
        "  return chart"
      ],
      "metadata": {
        "id": "uMLS8tDBA8C6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this plus the tokens++ returned by spaCy, we can pretty quickly examine some characteristics of the text."
      ],
      "metadata": {
        "id": "JNvzCvFfgHmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the 20 most-used words in the song.\n",
        "terms = [token.lower_ for token in one_song_doc if not (token.is_stop or token.is_punct or token.is_space)]\n",
        "frequency_chart(terms,20)"
      ],
      "metadata": {
        "id": "u30_yY1uAjI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the different parts of speech used\n",
        "terms = [token.pos_ for token in one_song_doc]\n",
        "frequency_chart(terms,20,normalize=False,color='orange')"
      ],
      "metadata": {
        "id": "PUK_Ud5EgrVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embeddings\n",
        "(If we're using a language model that supports it) spaCy can also generate vector embeddings for individual tokens. It can also return embeddings for documents (generated by averaging the vectors for the words in them).\n",
        "\n",
        "The assigned word vectors are based on how the words tend to occur in written text (as captured in spaCy's default corpus, **not** just on current document. This particular approach is efficient, but spaCy also lets you use a variety of other pretrained language models that can give better results. Lots more detail here: https://spacy.io/usage/embeddings-transformers"
      ],
      "metadata": {
        "id": "uWOmKXVshQTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's the first token in this song.\n",
        "print(one_song_doc[0])\n",
        "print(one_song_doc[0].vector.shape)\n"
      ],
      "metadata": {
        "id": "Wrd1UKKBEQTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And a n=300 embedding for it.\n",
        "one_song_doc[0].vector"
      ],
      "metadata": {
        "id": "7mgDRB1XicFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataframe with embeddings for all nouns in the song that occur in the corpus. The boolean `is_oov' indicates if a token is (\"out-of-vocabulary\") — if it is spaCy doesn't have the word in its corpus and can't produce a vector for it, so we'll ignore."
      ],
      "metadata": {
        "id": "6lEyKdqonf8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nouns = [token.lower_ for token in one_song_doc if not token.is_oov and not token.is_stop and (token.pos_ == \"PROPN\" or token.pos_ == 'NOUN')]\n",
        "noun_vecs = [token.vector for token in one_song_doc if not token.is_oov and not token.is_stop and (token.pos_ == \"PROPN\" or token.pos_ == 'NOUN')]\n",
        "noun_vecs_df = pd.DataFrame(noun_vecs)\n",
        "noun_vecs_df"
      ],
      "metadata": {
        "id": "-H-A-mWPikjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embeddings PCA\n",
        "Let's try projecting these into 2D space with PCA to have a look."
      ],
      "metadata": {
        "id": "clmON4DRoGUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "fJgFwaF1ikYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run PCA on the feature set dataframe\n",
        "pca = PCA(n_components = 2)\n",
        "noun_principle_components = pca.fit_transform(noun_vecs_df)"
      ],
      "metadata": {
        "id": "tbSsP1W5ikLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stick back into a DataFrame with the original labels and plot\n",
        "noun_pca = pd.DataFrame(noun_principle_components)\n",
        "noun_pca['word'] = nouns\n",
        "noun_pca.columns = ['pc1','pc2','word']\n",
        "noun_pca = noun_pca.groupby('word',as_index=False).mean()\n",
        "noun_pca"
      ],
      "metadata": {
        "id": "FdwoyUOlij_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot words\n",
        "scatter = alt.Chart(noun_pca).mark_point().encode(\n",
        "    x=\"pc1\",\n",
        "    y=\"pc2\",\n",
        "    tooltip=['pc1','pc2','word'])\n",
        "text = scatter.mark_text(align='left',baseline='middle', dx=10).encode(text=\"word\")\n",
        "(scatter + text).properties(width=600,height=600).configure_axis(grid=False)"
      ],
      "metadata": {
        "id": "CywnUGZX5BuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document-Level Embeddings\n",
        "We can also jump up a level and look at aggregated vectors for whole songs."
      ],
      "metadata": {
        "id": "7Fzf9izn2qJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1 — Just take spaCy's aggregated vectors for each song\n",
        "song_vecs = [nlp(song['lyrics']).vector for index,song in one_artist.iterrows()]\n",
        "song_vecs = pd.DataFrame(song_vecs)\n",
        "song_vecs.head()"
      ],
      "metadata": {
        "id": "G35iatKj8hup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 2  — Generate averaged vectors for each song using all the nouns (even repeats)\n",
        "song_vecs_nounsonly = []\n",
        "for index, song in one_artist.iterrows():\n",
        "  song_noun_vecs = [token.vector for token in nlp(song['lyrics']) if not token.is_oov and not token.is_stop and (token.pos_ == \"PROPN\" or token.pos_ == 'NOUN')]\n",
        "  song_vecs_nounsonly.append(pd.DataFrame(song_noun_vecs).mean())\n",
        "song_vecs_nounsonly = pd.DataFrame(song_vecs_nounsonly)\n",
        "song_vecs_nounsonly.head()"
      ],
      "metadata": {
        "id": "YKRJxI5U2qZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 3  — Generate averaged vectors for each song using all the nouns (no repeats)\n",
        "song_vecs_nounsonly_unique = []\n",
        "for index, song in one_artist.iterrows():\n",
        "  nouns_seen = set()\n",
        "  song_noun_vecs = []\n",
        "  for token in nlp(song['lyrics']):\n",
        "    if token.text not in nouns_seen and not token.is_oov and not token.is_stop and (token.pos_ == \"PROPN\" or token.pos_ == 'NOUN'):\n",
        "      nouns_seen.add(token.text)\n",
        "      song_noun_vecs.append(token.vector)\n",
        "  song_vecs_nounsonly_unique.append(pd.DataFrame(song_noun_vecs).mean())\n",
        "song_vecs_nounsonly_unique = pd.DataFrame(song_vecs_nounsonly_unique)\n",
        "song_vecs_nounsonly_unique.head()"
      ],
      "metadata": {
        "id": "_l9GZsAC6uzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run PCA to project our 300-dimensional space down to 2D\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "# Our three different Experiments\n",
        "songs_principle_components = pca.fit_transform(song_vecs)\n",
        "# songs_principle_components = pca.fit_transform(song_vecs_nounsonly)\n",
        "# songs_principle_components = pca.fit_transform(song_vecs_nounsonly_unique)\n",
        "\n",
        "songs_pca = pd.DataFrame(songs_principle_components)\n",
        "songs_pca.columns = ['pc1','pc2']\n",
        "songs_pca = pd.concat([songs_pca,one_artist],axis=1)\n",
        "songs_pca['lyrics_chars'] = songs_pca['lyrics'].map(lambda l: len(str(l)))\n",
        "songs_pca.sample(3)"
      ],
      "metadata": {
        "id": "I8eRPYkx2qmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot songs\n",
        "scatter = alt.Chart(songs_pca).mark_point().encode(\n",
        "    x='pc1',\n",
        "    y='pc2',\n",
        "    size='lyrics_chars',\n",
        "    color=alt.Color('source:N',scale=alt.Scale(scheme='category20')),\n",
        "    tooltip=['pc1','pc2','title','artist_names','album'],\n",
        "    href='url')\n",
        "text = alt.Chart(songs_pca).mark_text(align='left',baseline='middle', dx=10).encode(\n",
        "    x='pc1', y='pc2', text=\"title\")\n",
        "(scatter+text).properties(width=600,height=600).configure_axis(grid=False)"
      ],
      "metadata": {
        "id": "8PeaGaUl2qyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semantic Similarity\n",
        "spaCy can also perform pairwise document similarity comparisons using word vectors directly."
      ],
      "metadata": {
        "id": "LNYkSd3LHEd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amish_paradise = nlp(one_artist.iloc[0]['lyrics'])\n",
        "albuquerque = nlp(one_artist.iloc[2]['lyrics'])\n",
        "yoda = nlp(one_artist.iloc[11]['lyrics'])\n",
        "\n",
        "print('\"Amish Paradise\" semantic similarity to..')\n",
        "print(' \"Albuquerque\":', amish_paradise.similarity(albuquerque))\n",
        "print(' \"Yoda\":', amish_paradise.similarity(yoda))"
      ],
      "metadata": {
        "id": "sBdhQd9sHRY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many additional approaches you can try here.\n",
        "\n",
        "See the spaCy docs and intro course here: https://spacy.io/usage/spacy-101"
      ],
      "metadata": {
        "id": "AE3TOOlqF_rS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Fd5y7_0GDW5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}